{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "\n",
    "## Car Price Prediction\n",
    "\n",
    "Predict if the price of a car is low or high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Angie Paola Chacon Pinzon: 201012536\n",
    "#### Jahir Stevens Rodriguez Riveros: 201819361\n",
    "#### Juan Andres Cabal Jaramillo: 200620778\n",
    "#### David Tavera SÃ¡nchez: 201016123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>M_Camry</th>\n",
       "      <th>M_Camry4dr</th>\n",
       "      <th>M_CamryBase</th>\n",
       "      <th>M_CamryL</th>\n",
       "      <th>M_CamryLE</th>\n",
       "      <th>M_CamrySE</th>\n",
       "      <th>M_CamryXLE</th>\n",
       "      <th>HighPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016</td>\n",
       "      <td>29242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2015</td>\n",
       "      <td>26465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2012</td>\n",
       "      <td>46739</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2017</td>\n",
       "      <td>41722</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2014</td>\n",
       "      <td>77669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Mileage  M_Camry  M_Camry4dr  M_CamryBase  M_CamryL  M_CamryLE  \\\n",
       "15   2016    29242        0           0            0         0          1   \n",
       "47   2015    26465        0           0            0         0          1   \n",
       "85   2012    46739        0           1            0         0          0   \n",
       "141  2017    41722        0           0            0         0          0   \n",
       "226  2014    77669        0           0            0         0          0   \n",
       "\n",
       "     M_CamrySE  M_CamryXLE  HighPrice  \n",
       "15           0           0          1  \n",
       "47           0           0          1  \n",
       "85           0           0          1  \n",
       "141          1           0          1  \n",
       "226          0           1          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv('./datasets/dataTrain_carListings.zip')\n",
    "data = data.loc[data['Model'].str.contains('Camry')].drop(['Make', 'State'], axis=1)\n",
    "data = data.join(pd.get_dummies(data['Model'], prefix='M'))\n",
    "data['HighPrice'] = (data['Price'] > data['Price'].mean()).astype(int)\n",
    "data = data.drop(['Model', 'Price'], axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13150, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['HighPrice']\n",
    "X = data.drop(['HighPrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.1\n",
    "\n",
    "Estimate a Decision Tree Classifier Manually using the code created in the Notebook #13\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    if y.shape[0] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 - (y.mean()**2 + (1 - y.mean())**2)\n",
    "    \n",
    "def gini_impurity(X_col, y, split):\n",
    "    \"Calculate the gain of an split k on feature j\"\n",
    "    \n",
    "    filter_l = X_col < split\n",
    "    y_l = y.loc[filter_l]\n",
    "    y_r = y.loc[~filter_l]\n",
    "    \n",
    "    n_l = y_l.shape[0]\n",
    "    n_r = y_r.shape[0]\n",
    "    \n",
    "    gini_y = gini(y)\n",
    "    gini_l = gini(y_l)\n",
    "    gini_r = gini(y_r)\n",
    "    \n",
    "    gini_impurity_ = gini_y - (n_l / (n_l + n_r) * gini_l + n_r / (n_l + n_r) * gini_r)\n",
    "    \n",
    "    return gini_impurity_\n",
    "\n",
    "def best_split(X, y, num_pct=10):\n",
    "    \n",
    "    features = range(X.shape[1])\n",
    "    \n",
    "    best_split = [0, 0, 0]  # j, split, gain\n",
    "    \n",
    "    # For all features\n",
    "    for j in features:\n",
    "        \n",
    "        splits = np.percentile(X.iloc[:, j], np.arange(0, 100, 100.0 / (num_pct+1)).tolist())\n",
    "        splits = np.unique(splits)[1:]\n",
    "        \n",
    "        # For all splits\n",
    "        for split in splits:\n",
    "            gain = gini_impurity(X.iloc[:, j], y, split)\n",
    "            #print(gain)            \n",
    "            if gain > best_split[2]:\n",
    "                best_split = [j, split, gain]\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "def tree_grow(X, y, level=0, min_gain=0.001, max_depth=None, num_pct=10):\n",
    "    \n",
    "    # If only one observation\n",
    "    if X.shape[0] == 1:\n",
    "        tree = dict(y_pred=y.iloc[:1].values[0], y_prob=0.5, level=level, split=-1, n_samples=1, gain=0)\n",
    "        return tree\n",
    "    \n",
    "    # Calculate the best split\n",
    "    j, split, gain = best_split(X, y, num_pct)\n",
    "    \n",
    "    # save tree and estimate prediction\n",
    "    y_pred = int(y.mean() >= 0.5) \n",
    "    y_prob = (y.sum() + 1.0) / (y.shape[0] + 2.0)  # Laplace correction\n",
    "    \n",
    "    tree = dict(y_pred=y_pred, y_prob=y_prob, level=level, split=-1, n_samples=X.shape[0], gain=gain)\n",
    "    \n",
    "    # Check stooping criteria\n",
    "    if gain < min_gain:\n",
    "        return tree\n",
    "    if max_depth is not None:\n",
    "        if level >= max_depth:\n",
    "            return tree   \n",
    "    \n",
    "    # No stooping criteria was meet, then continue to create the partition\n",
    "    filter_l = X.iloc[:, j] < split\n",
    "    X_l, y_l = X.loc[filter_l], y.loc[filter_l]\n",
    "    X_r, y_r = X.loc[~filter_l], y.loc[~filter_l]\n",
    "    tree['split'] = [j, split]\n",
    "\n",
    "    # Next iteration to each split\n",
    "    \n",
    "    tree['sl'] = tree_grow(X_l, y_l, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    tree['sr'] = tree_grow(X_r, y_r, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    \n",
    "    return tree \n",
    "def tree_predict(X, tree, proba=False):\n",
    "    \n",
    "    predicted = np.ones(X.shape[0])\n",
    "\n",
    "    # Check if final node\n",
    "    if tree['split'] == -1:\n",
    "        if not proba:\n",
    "            predicted = predicted * tree['y_pred']\n",
    "        else:\n",
    "            predicted = predicted * tree['y_prob']\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        j, split = tree['split']\n",
    "        filter_l = (X.iloc[:, j] < split)\n",
    "        X_l = X.loc[filter_l]\n",
    "        X_r = X.loc[~filter_l]\n",
    "\n",
    "        if X_l.shape[0] == 0:  # If left node is empty only continue with right\n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'], proba)\n",
    "        elif X_r.shape[0] == 0:  # If right node is empty only continue with left\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'], proba)\n",
    "        else:\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'], proba)\n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'], proba)\n",
    "\n",
    "    return predicted    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8610599078341014\n",
      "F1-Score: 0.882798833819242\n"
     ]
    }
   ],
   "source": [
    "tree = tree_grow(X_train, y_train, level=0, min_gain=0.001, max_depth=3, num_pct=10)\n",
    "y_hat=tree_predict(X_test, tree)\n",
    "#y_hat[:4]\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_hat,y_test))\n",
    "print(\"F1-Score:\", metrics.f1_score(y_hat,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.2\n",
    "\n",
    "Estimate a Bagging of 10 Decision Tree Classifiers Manually using the code created in the Notebook #7\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 235, 5192,  905, ..., 6990, 3970, 4337]),\n",
       " array([ 695, 2764, 6180, ..., 7495, 4266, 3256]),\n",
       " array([6434, 8533, 3845, ..., 8624, 2779, 1284]),\n",
       " array([6607,  682, 1153, ..., 4502, 3684, 8586]),\n",
       " array([5267, 1878, 3085, ..., 2889, 2678, 5273]),\n",
       " array([5564, 3084, 4061, ..., 3010, 3961, 5753]),\n",
       " array([3836,  646, 1289, ...,  608, 7604, 8458]),\n",
       " array([1201, 5768, 5392, ..., 1500, 1119, 3696]),\n",
       " array([4334, 3801, 2516, ..., 4745, 4402,  503]),\n",
       " array([3795, 4635, 6140, ..., 7759, 2223, 2405])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_B = 10\n",
    "\n",
    "samples = [np.random.choice(a = n_samples, size = n_samples, replace = True) for _ in range(1, n_B + 1 )]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(index=y_test.index, columns=[list(range(n_B))])\n",
    "\n",
    "for i  in range(1,n_B):\n",
    "    X = X_train.iloc[samples[i], 0:]\n",
    "    y = y_train.iloc[samples[i]]\n",
    "    tree = tree_grow(X, y, level=0, min_gain=0.001, max_depth=3, num_pct=10)\n",
    "    y_pred[[i]] = tree_predict(X_test, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8700460829493087\n",
      "F1-Score: 0.8923253150057274\n"
     ]
    }
   ],
   "source": [
    "bag_pred = (y_pred.sum(axis = 1) >= (n_B / 2)).astype(np.int)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, bag_pred))\n",
    "print(\"F1-Score:\", metrics.f1_score(y_test, bag_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.3\n",
    "\n",
    "Implement the variable max_features on the Decision Tree Classifier created in 11.1.\n",
    "\n",
    "Compare the impact in the results by varing the parameter max_features\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_grow_r(X, y, level=0, min_gain=0.001, max_depth=None, num_pct=10, max_features=None):\n",
    "    #print(\"tr\",max_features)\n",
    "    # If only one observation\n",
    "    if X.shape[0] == 1:\n",
    "        tree = dict(y_pred=y.iloc[:1].values[0], y_prob=0.5, level=level, split=-1, n_samples=1, gain=0)\n",
    "        return tree\n",
    "    \n",
    "    # select random columns\n",
    "    X_rdn=X.sample(max_features, axis=1)\n",
    "    #print(\"Xrdn:\",X_rdn.head())\n",
    "    # Calculate the best split\n",
    "    j_rdn, split, gain = best_split(X_rdn, y, num_pct)\n",
    "    \n",
    "    j=X_train.columns.get_loc(X_rdn.columns[j_rdn])\n",
    "    \n",
    "    # save tree and estimate prediction\n",
    "    y_pred = int(y.mean() >= 0.5) \n",
    "    y_prob = (y.sum() + 1.0) / (y.shape[0] + 2.0)  # Laplace correction\n",
    "    \n",
    "    tree = dict(y_pred=y_pred, y_prob=y_prob, level=level, split=-1, n_samples=X.shape[0], gain=gain)\n",
    "    \n",
    "    # Check stooping criteria\n",
    "    if gain < min_gain:\n",
    "        return tree\n",
    "    if max_depth is not None:\n",
    "        if level >= max_depth:\n",
    "            return tree   \n",
    "    \n",
    "    # No stooping criteria was meet, then continue to create the partition\n",
    "    filter_l = X.iloc[:, j] < split\n",
    "    X_l, y_l = X.loc[filter_l], y.loc[filter_l]\n",
    "    X_r, y_r = X.loc[~filter_l], y.loc[~filter_l]\n",
    "    tree['split'] = [j, split]\n",
    "\n",
    "    # Next iteration to each split\n",
    "    \n",
    "    tree['sl'] = tree_grow_r(X_l, y_l, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct,max_features=max_features)\n",
    "    tree['sr'] = tree_grow_r(X_r, y_r, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct,max_features=max_features)\n",
    "    \n",
    "    return tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxf: 1  acc: 0.6612903225806451\n",
      "maxf: 2  acc: 0.743778801843318\n",
      "maxf: 3  acc: 0.5824884792626728\n",
      "maxf: 4  acc: 0.8354838709677419\n",
      "maxf: 5  acc: 0.8564516129032258\n",
      "maxf: 6  acc: 0.8656682027649769\n",
      "maxf: 7  acc: 0.864516129032258\n",
      "maxf: 8  acc: 0.8626728110599078\n",
      "maxf: 9  acc: 0.8610599078341014\n"
     ]
    }
   ],
   "source": [
    "for m in range(1,10):\n",
    "    tree_r = tree_grow_r(X_train, y_train, level=0, min_gain=0.001, max_depth=3, num_pct=10 ,max_features=m)\n",
    "    y_hat_r=tree_predict(X_test, tree_r)\n",
    "    print(\"maxf:\",m,\" acc:\",metrics.accuracy_score(y_hat_r,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.4\n",
    "\n",
    "Estimate a Bagging of 10 Decision Tree Classifiers with `max_features = log(n_features)`\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mx=int(np.log(len(X.columns)))\n",
    "np.random.seed(1)\n",
    "n_samples = X_train.shape[0]\n",
    "n_B = 10\n",
    "samples_m = [np.random.choice(a = n_samples, size = n_samples, replace = True) for _ in range(1, n_B + 1 )]\n",
    "y_pred_m = pd.DataFrame(index=y_test.index, columns=[list(range(n_B))])\n",
    "\n",
    "for i  in range(1,n_B):\n",
    "    X_m = X_train.iloc[samples_m[i], 0:]\n",
    "    y_m = y_train.iloc[samples_m[i]]\n",
    "    tree_m = tree_grow(X_m, y_m, level=0, min_gain=0.001, max_depth=mx, num_pct=10)\n",
    "    y_pred_m[[i]] = tree_predict(X_test, tree_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8610599078341014\n",
      "F1-Score: 0.882798833819242\n"
     ]
    }
   ],
   "source": [
    "bag_pred_m = (y_pred_m.sum(axis = 1) >= (n_B / 2)).astype(np.int)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, bag_pred_m))\n",
    "print(\"F1-Score:\", metrics.f1_score(y_test, bag_pred_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.5\n",
    "\n",
    "Using sklearn, train a RandomForestClassifier\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.845852534562212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_features=7,random_state=1, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.6\n",
    "\n",
    "Find the best parameters of the RandomForestClassifier (max_depth, max_features, n_estimators)\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of values to try for max_features\n",
    "depth_range = range(1,5)\n",
    "feature_range = range(1, len(X_train.columns)+1)\n",
    "estimator_range = range(10, 20, 5)\n",
    "\n",
    "# list to store the average Accuracy for each value of max_features\n",
    "accuracy_scores = []\n",
    "\n",
    "\n",
    "for feature in feature_range:\n",
    "    for depth in depth_range:\n",
    "        for estim in estimator_range:\n",
    "            clf = RandomForestClassifier(n_estimators=estim, max_depth=depth,max_features=feature, random_state=1, n_jobs=-1)\n",
    "            #clf.fit(X_train,y_train)\n",
    "            #y_pred=clf.predict(X_test)\n",
    "            #acc=metrics.accuracy_score(y_pred,y_test)\n",
    "            cv_acc=cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "            accuracy_scores.append([feature,depth,estim,cv_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.877185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f  d   e       acc\n",
       "23  3  4  15  0.877185"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc=pd.DataFrame(accuracy_scores,columns=['f','d','e','acc'])\n",
    "bm=df_acc[df_acc['acc']==df_acc['acc'].max()]\n",
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=bm['e'].values[0], max_depth=bm['d'].values[0],\n",
    "                             max_features=bm['f'].values[0], random_state=1, n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "y_pred\n",
    "#print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.7 \n",
    "\n",
    "Using xgboost train a XGBClassifier \n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8917995444191343, 0.868663594470046)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clfXG = XGBClassifier()\n",
    "clfXG.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.8\n",
    "\n",
    "Using xgboost train a XGBClassifier \n",
    "\n",
    "Modify the parameters learning rate, gamma, colsample_bytree. Explain what each parameter means.\n",
    "\n",
    "Evaluate the accuracy on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning rate:  0<ðâ¤1. FracciÃ³n de los componentes de un Ã¡rbol que serÃ¡n usadas para la estimaciÃ³n del siguiente Ã¡rbol. Es usado para mejorar la generalizaciÃ³n del Ã¡rbol. Cuando mÃ¡s pequeÃ±o es el parÃ¡metro es mejor la generalizaciÃ³n que hace. CuÃ¡ndo el valor es 1, significa que se usan todos los componentes del Ã¡rbol actual para crear el siguiente Ã¡rbol\n",
    "- gamma: Ganancia mÃ­nima que debe superar una particiÃ³n para ser incluida en el Ã¡rbol. Es un nÃºmero entre [0,inf] y a medida que es mÃ¡s grande el Ã¡rbol final es mÃ¡s simple.\n",
    "- colsample_bytree: FracciÃ³n de las columnas que serÃ¡n tomadas para la construcciÃ³n de cada Ã¡rbol. tiene un valor entre 0 y 1 siendo 1:\"tomar todas las columnas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clfXG = XGBClassifier()\n",
    "clfXG.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of values to try for max_features\n",
    "learning_range = np.arange(0, 1, 0.1)\n",
    "gamma_range = range(1, 10)\n",
    "colsample_range =np.arange(0, 1, 0.1)\n",
    "\n",
    "# list to store the average Accuracy for each value of max_features\n",
    "accuracy_scores = []\n",
    "\n",
    "for lr in learning_range:\n",
    "    for g in gamma_range:\n",
    "        for cs in colsample_range:\n",
    "            clf = XGBClassifier(eta=lr, gamma=g,colsample_bytree=cs)\n",
    "            clf.fit(X_train,y_train)\n",
    "            y_pred=clf.predict(X_test)\n",
    "            acc=metrics.accuracy_score(y_pred,y_test)\n",
    "            accuracy_scores.append([lr,g,cs,acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>g</th>\n",
       "      <th>cs</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.885023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lr  g   cs       acc\n",
       "396  0.4  4  0.6  0.885023"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfXB_acc=pd.DataFrame(accuracy_scores,columns=['lr','g','cs','acc'])\n",
    "bmXG=dfXB_acc[dfXB_acc['acc']==dfXB_acc['acc'].max()]\n",
    "bmXG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
